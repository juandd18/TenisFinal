{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment and Imports\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from model import DDPGAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Tennis_Windows_x86_64/Tennis.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit current run\n",
    "curr_run = 'run6'\n",
    "model_dir = Path('./models') \n",
    "run_dir = model_dir / curr_run\n",
    "log_dir = run_dir / 'logs'\n",
    "\n",
    "#os.makedirs(run_dir)\n",
    "#os.makedirs(log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set the parameters\n",
    "\n",
    "in this section we set the parameter to use in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of training episodes.\n",
    "number_of_episodes = 4000\n",
    "episode_length = 50\n",
    "batch_size = 120\n",
    "\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "noise = 1\n",
    "noise_reduction = 0.9999\n",
    "\n",
    "hidden_dim_actor = 256\n",
    "hidden_dim_critic = 256\n",
    "gamma=0.95\n",
    "tau=0.001\n",
    "lr_actor=0.001\n",
    "lr_critic=0.001\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "agent_init_params = {'num_in_pol': state_size,\n",
    "                     'num_out_pol': action_size,\n",
    "                     'num_in_critic': state_size,\n",
    "                     'hidden_dim_actor': hidden_dim_actor,\n",
    "                     'hidden_dim_critic': hidden_dim_critic,\n",
    "                     'tau':tau,\n",
    "                     'gamma': gamma,\n",
    "                    'lr_actor':lr_actor,\n",
    "                    'lr_critic':lr_critic,\n",
    "                    'batch_size':batch_size,\n",
    "                    'max_episode_len':episode_length}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model\n",
    "\n",
    "in this section we train the maddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent1 = DDPGAgent(**agent_init_params)\n",
    "agent2 = DDPGAgent(**agent_init_params)\n",
    "\n",
    "\n",
    "# load the pretained model\n",
    "\n",
    "agent_0_weights = run_dir  /  'checkpoint_actor1.pth'\n",
    "agent_0_critic_weights =  run_dir  /  'checkpoint_critic1.pth'\n",
    "agent_0_actor_target =  run_dir  /  'checkpoint_actor_target1.pth'\n",
    "agent_0_critic_target =   run_dir  / 'checkpoint_critic_target1.pth'\n",
    "\n",
    "\n",
    "agent_1_weights =  run_dir  /  'checkpoint_actor2.pth'\n",
    "agent_1_critic_weights = run_dir  /  'checkpoint_critic2.pth'\n",
    "agent_1_actor_target = run_dir  /   'checkpoint_actor_target2.pth'\n",
    "agent_1_critic_target = run_dir  /   'checkpoint_critic_target2.pth'\n",
    "\n",
    "agent1.policy.load_state_dict(torch.load(agent_0_weights))\n",
    "agent1.critic.load_state_dict(torch.load(agent_0_critic_weights))\n",
    "agent1.target_policy.load_state_dict(torch.load(agent_0_actor_target))\n",
    "agent1.target_critic.load_state_dict(torch.load(agent_0_critic_target))\n",
    "\n",
    "agent2.policy.load_state_dict(torch.load(agent_1_weights))\n",
    "agent2.critic.load_state_dict(torch.load(agent_1_critic_weights))\n",
    "agent2.target_policy.load_state_dict(torch.load(agent_1_actor_target))\n",
    "agent2.target_critic.load_state_dict(torch.load(agent_1_critic_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\t Mean Score of both Agents: 0.0000, replay memory0 11.000, repleay mem1 11.000 \n",
      "Episode 200\t Mean Score of both Agents: 0.0020, replay memory0 3007.000, repleay mem1 3007.000 \n",
      "Episode 400\t Mean Score of both Agents: 0.0010, replay memory0 6030.000, repleay mem1 6030.000 \n",
      "Episode 600\t Mean Score of both Agents: 0.0000, replay memory0 8870.000, repleay mem1 8870.000 \n",
      "Episode 800\t Mean Score of both Agents: 0.0000, replay memory0 11778.000, repleay mem1 11778.000 \n",
      "Episode 1000\t Mean Score of both Agents: 0.0270, replay memory0 15526.000, repleay mem1 15526.000 \n",
      "Episode 1200\t Mean Score of both Agents: 0.1930, replay memory0 22857.000, repleay mem1 22857.000 \n",
      "Episode 1400\t Mean Score of both Agents: 0.3860, replay memory0 35583.000, repleay mem1 35583.000 \n",
      "Episode 1600\t Mean Score of both Agents: 0.3710, replay memory0 50455.000, repleay mem1 50455.000 \n",
      "Episode 1800\t Mean Score of both Agents: 1.0300, replay memory0 85968.000, repleay mem1 85968.000 \n",
      "Episode 1900\t Mean Score of both Agents: 1.2668, replay memory0 111289.000, repleay mem1 111289.000 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0013ccf2c1cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0magent1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0magent2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\reinforcement\\Tenis\\model\\ddpgv2.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done, t_step)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mq_targets_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mq_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mq_targets_next\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mq_expected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\reinforcement\\Tenis\\model\\ddpgv2.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, experiences, gamma)\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\reinforcement\\Tenis\\model\\ddpgv2.py\u001b[0m in \u001b[0;36msoft_update\u001b[1;34m(self, local_model, target_model, tau)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#agent1 = DDPGAgent(**agent_init_params)\n",
    "#agent2 = DDPGAgent(**agent_init_params)\n",
    "\n",
    "scores = []\n",
    "mean_scores = []\n",
    "\n",
    "agent1.policy.cuda()\n",
    "agent1.target_policy.cuda()\n",
    "\n",
    "t_step = 0\n",
    "\n",
    "for i_episode in range(0, number_of_episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "    \n",
    "    agent1.reset_noise()\n",
    "    agent2.reset_noise()\n",
    "    score = 0\n",
    "    \n",
    "    while True:\n",
    "        state1 =  state[0]\n",
    "        state2 = state[1] \n",
    "        \n",
    "        action1 = agent1.act(state1.reshape((1,state1.shape[0])),explore=True)\n",
    "        action2 = agent2.act(state2.reshape((1,state2.shape[0])),explore=True)\n",
    "             \n",
    "        env_info = env.step([action1, action2])[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        next_state1 = next_state[0] \n",
    "        next_state2 = next_state[1] \n",
    "        \n",
    "        agent1.step(state1, action1, reward[0],next_state1, done,t_step)\n",
    "        agent2.step(state2, action2, reward[1],next_state2, done,t_step)\n",
    "        \n",
    "        state = next_state\n",
    "        score += np.max(reward)\n",
    "        t_step+=1\n",
    "        \n",
    "        if np.any(done):\n",
    "            break\n",
    "            \n",
    "    scores.append(score)\n",
    "    mean = np.mean(scores[-100:])\n",
    "\n",
    "    print('\\rEpisode {}\\t Mean Score of both Agents: {:.4f}, replay memory0 {:.3f}, repleay mem1 {:.3f} '.format(i_episode,  mean,len(agent1.replay_buffer),\n",
    "                                                                                                                                                 len(agent2.replay_buffer)),end=\"\")\n",
    "    #print('\\rstep {}'.format(t_step),end=\"\")\n",
    "    if i_episode % 200 ==0:\n",
    "        #logger.add_scalar('agent%i/mean_episode_rewards' % i_episode,  mean_score)\n",
    "        print('\\rEpisode {}\\t Mean Score of both Agents: {:.4f}, replay memory0 {:.3f}, repleay mem1 {:.3f} '.format(i_episode,  mean,len(agent1.replay_buffer),\n",
    "                                                                                                                                                 len(agent2.replay_buffer)))\n",
    "    \n",
    "    if i_episode % 100 ==0:\n",
    "        print('\\rEpisode {}\\t Mean Score of both Agents: {:.4f}, replay memory0 {:.3f}, repleay mem1 {:.3f} '.format(i_episode,  mean,len(agent1.replay_buffer),\n",
    "                                                                                                                                                 len(agent2.replay_buffer)),end=\"\")\n",
    "        torch.save(agent1.policy.state_dict(), run_dir / 'checkpoint_actor1.pth')\n",
    "        torch.save(agent1.critic.state_dict(), run_dir / 'checkpoint_critic1.pth')\n",
    "        torch.save(agent1.target_policy.state_dict(),run_dir / 'checkpoint_actor_target1.pth')\n",
    "        torch.save(agent1.target_critic.state_dict(),run_dir / 'checkpoint_critic_target1.pth')\n",
    "            \n",
    "        torch.save(agent2.policy.state_dict(), run_dir /'checkpoint_actor2.pth')\n",
    "        torch.save(agent2.critic.state_dict(),run_dir / 'checkpoint_critic2.pth')\n",
    "        torch.save(agent2.target_policy.state_dict(),run_dir / 'checkpoint_actor_target2.pth')\n",
    "        torch.save(agent2.target_critic.state_dict(),run_dir / 'checkpoint_critic_target2.pth')\n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\t Mean Score of both Agents: 0.0000, replay memory0 15.000, repleay mem1 15.000 \n",
      "Episode 200\t Mean Score of both Agents: 0.0330, replay memory0 3604.000, repleay mem1 3604.000 \n",
      "Episode 400\t Mean Score of both Agents: 0.0010, replay memory0 6782.000, repleay mem1 6782.000 \n",
      "Episode 600\t Mean Score of both Agents: 0.0010, replay memory0 9738.000, repleay mem1 9738.000 \n",
      "Episode 800\t Mean Score of both Agents: 0.0020, replay memory0 12633.000, repleay mem1 12633.000 \n",
      "Episode 1000\t Mean Score of both Agents: 0.0380, replay memory0 16348.000, repleay mem1 16348.000 \n",
      "Episode 1200\t Mean Score of both Agents: 0.0618, replay memory0 21833.000, repleay mem1 21833.000 \n",
      "Episode 1400\t Mean Score of both Agents: 0.1279, replay memory0 28502.000, repleay mem1 28502.000 \n",
      "Episode 1600\t Mean Score of both Agents: 1.0039, replay memory0 56166.000, repleay mem1 56166.000 \n",
      "Episode 1704\t Mean Score of both Agents: 2.1500, replay memory0 99207.000, repleay mem1 99207.000 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3c541d87e2cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0magent1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0magent2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\reinforcement\\TenisFinal\\model\\ddpgv2.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done, t_step)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\reinforcement\\TenisFinal\\model\\ddpgv2.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.51\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mpredicted_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = []\n",
    "mean_scores = []\n",
    "\n",
    "agent1.policy.cuda()\n",
    "agent1.target_policy.cuda()\n",
    "\n",
    "t_step = 0\n",
    "\n",
    "for i_episode in range(0, number_of_episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "    \n",
    "    agent1.reset_noise()\n",
    "    agent2.reset_noise()\n",
    "    score = 0\n",
    "    \n",
    "    while True:\n",
    "        state1 =  state[0]\n",
    "        state2 = state[1] \n",
    "        \n",
    "        action1 = agent1.act(state1.reshape((1,state1.shape[0])),explore=True)\n",
    "        action2 = agent2.act(state2.reshape((1,state2.shape[0])),explore=True)\n",
    "             \n",
    "        env_info = env.step([action1, action2])[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        next_state1 = next_state[0] \n",
    "        next_state2 = next_state[1] \n",
    "        \n",
    "        agent1.step(state1, action1, reward[0],next_state1, done,t_step)\n",
    "        agent2.step(state2, action2, reward[1],next_state2, done,t_step)\n",
    "        \n",
    "        state = next_state\n",
    "        score += np.max(reward)\n",
    "        t_step+=1\n",
    "        \n",
    "        if np.any(done):\n",
    "            break\n",
    "            \n",
    "    scores.append(score)\n",
    "    mean = np.mean(scores[-100:])\n",
    "\n",
    "    print('\\rEpisode {}\\t Mean Score of both Agents: {:.4f}, replay memory0 {:.3f}, repleay mem1 {:.3f} '.format(i_episode,  mean,len(agent1.replay_buffer),\n",
    "                                                                                                                                                 len(agent2.replay_buffer)),end=\"\")\n",
    "    #print('\\rstep {}'.format(t_step),end=\"\")\n",
    "    if i_episode % 200 ==0:\n",
    "        #logger.add_scalar('agent%i/mean_episode_rewards' % i_episode,  mean_score)\n",
    "        print('\\rEpisode {}\\t Mean Score of both Agents: {:.4f}, replay memory0 {:.3f}, repleay mem1 {:.3f} '.format(i_episode,  mean,len(agent1.replay_buffer),\n",
    "                                                                                                                                                 len(agent2.replay_buffer)))\n",
    "    \n",
    "    if i_episode % 100 ==0:\n",
    "        print('\\rEpisode {}\\t Mean Score of both Agents: {:.4f}, replay memory0 {:.3f}, repleay mem1 {:.3f} '.format(i_episode,  mean,len(agent1.replay_buffer),\n",
    "                                                                                                                                                 len(agent2.replay_buffer)),end=\"\")\n",
    "        torch.save(agent1.policy.state_dict(), run_dir / 'checkpoint_actor13.pth')\n",
    "        torch.save(agent1.critic.state_dict(), run_dir / 'checkpoint_critic13.pth')\n",
    "        torch.save(agent1.target_policy.state_dict(),run_dir / 'checkpoint_actor_target13.pth')\n",
    "        torch.save(agent1.target_critic.state_dict(),run_dir / 'checkpoint_critic_target13.pth')\n",
    "            \n",
    "        torch.save(agent2.policy.state_dict(), run_dir /'checkpoint_actor23.pth')\n",
    "        torch.save(agent2.critic.state_dict(),run_dir / 'checkpoint_critic23.pth')\n",
    "        torch.save(agent2.target_policy.state_dict(),run_dir / 'checkpoint_actor_target23.pth')\n",
    "        torch.save(agent2.target_critic.state_dict(),run_dir / 'checkpoint_critic_target23.pth')\n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='MADDPG')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Tennis_Windows_x86_64/Tennis.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1_pre = DDPGAgent(**agent_init_params)\n",
    "agent2_pre = DDPGAgent(**agent_init_params)\n",
    "\n",
    "\n",
    "# load the pretained model\n",
    "\n",
    "agent_0_weights = run_dir  /  'checkpoint_actor13.pth'\n",
    "agent_0_critic_weights =  run_dir  /  'checkpoint_critic13.pth'\n",
    "agent_0_actor_target =  run_dir  /  'checkpoint_actor_target13.pth'\n",
    "agent_0_critic_target =   run_dir  / 'checkpoint_critic_target13.pth'\n",
    "\n",
    "\n",
    "agent_1_weights =  run_dir  /  'checkpoint_actor23.pth'\n",
    "agent_1_critic_weights = run_dir  /  'checkpoint_critic23.pth'\n",
    "agent_1_actor_target = run_dir  /   'checkpoint_actor_target23.pth'\n",
    "agent_1_critic_target = run_dir  /   'checkpoint_critic_target23.pth'\n",
    "\n",
    "agent1_pre.policy.load_state_dict(torch.load(agent_0_weights))\n",
    "agent1_pre.critic.load_state_dict(torch.load(agent_0_critic_weights))\n",
    "agent1_pre.target_policy.load_state_dict(torch.load(agent_0_actor_target))\n",
    "agent1_pre.target_critic.load_state_dict(torch.load(agent_0_critic_target))\n",
    "\n",
    "agent2_pre.policy.load_state_dict(torch.load(agent_1_weights))\n",
    "agent2_pre.critic.load_state_dict(torch.load(agent_1_critic_weights))\n",
    "agent2_pre.target_policy.load_state_dict(torch.load(agent_1_actor_target))\n",
    "agent2_pre.target_critic.load_state_dict(torch.load(agent_1_critic_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.6000000089406967\n",
      "Score (max over agents) from episode 2: 2.600000038743019\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.10000000149011612\n",
      "Score (max over agents) from episode 5: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        \n",
    "        state1 =states[0]\n",
    "        state2 = states[1] \n",
    "        \n",
    "        action1 = agent1_pre.act(state1.reshape((1,state1.shape[0])),explore=False)\n",
    "        action2 = agent2_pre.act(state2.reshape((1,state2.shape[0])),explore=False)\n",
    "        \n",
    "        env_info = env.step([action1, action2])[brain_name]\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
